{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwlin/miniconda3/envs/gnn4rec/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from tqdm import trange\n",
    "import random\n",
    "from itertools import combinations\n",
    "from torch_geometric.nn import GATConv\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_parameters(named_parameters):\n",
    "    for i in named_parameters():\n",
    "        if len(i[1].size()) == 1:\n",
    "            std = 1.0 / math.sqrt(i[1].size(0))\n",
    "            nn.init.uniform_(i[1], -std, std)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(i[1])\n",
    "\n",
    "class Graph_Linear(nn.Module):\n",
    "    def __init__(self,num_nodes, input_size, hidden_size, bias=True):\n",
    "        super(Graph_Linear, self).__init__()\n",
    "        self.bias = bias\n",
    "        self.W = nn.Parameter(torch.zeros(num_nodes, input_size,hidden_size))\n",
    "        self.b = nn.Parameter(torch.zeros(num_nodes, hidden_size))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset_parameters(self.named_parameters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.bmm(x.unsqueeze(1), self.W)\n",
    "        output = output.squeeze(1)\n",
    "        if self.bias:\n",
    "            output = output + self.b\n",
    "        return output\n",
    "\n",
    "class Graph_GRUCell(nn.Module):\n",
    "    def __init__(self, num_nodes, input_size, hidden_size, bias=True):\n",
    "        super(Graph_GRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.x2h = Graph_Linear(num_nodes, input_size, 3 * hidden_size, bias=bias)\n",
    "        self.h2h = Graph_Linear(num_nodes, hidden_size, 3 * hidden_size, bias=bias)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset_parameters(self.named_parameters)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        gate_x = self.x2h(x)\n",
    "        gate_h = self.h2h(hidden)\n",
    "        # 不知道為啥要comment這邊\n",
    "        # gate_x = gate_x.squeeze()\n",
    "        # gate_h = gate_h.squeeze()\n",
    "        i_r, i_i, i_n = gate_x.chunk(3, 1)\n",
    "        h_r, h_i, h_n = gate_h.chunk(3, 1)\n",
    "        resetgate = torch.sigmoid(i_r + h_r)\n",
    "        inputgate = torch.sigmoid(i_i + h_i)\n",
    "        newgate = torch.tanh(i_n + (resetgate * h_n))\n",
    "        hy = newgate + inputgate * (hidden - newgate)\n",
    "        return hy\n",
    "\n",
    "class Graph_GRUModel(nn.Module):\n",
    "    def __init__(self, num_nodes, input_dim, hidden_dim, bias=True):\n",
    "        super(Graph_GRUModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru_cell = Graph_GRUCell(num_nodes, input_dim, hidden_dim)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset_parameters(self.named_parameters)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(x.size()[1], self.hidden_dim, device=x.device,dtype = x.dtype)\n",
    "        for seq in range(x.size(0)):\n",
    "            hidden = self.gru_cell(x[seq], hidden)\n",
    "        return hidden\n",
    "\n",
    "class CategoricalGraphAtt(nn.Module):\n",
    "    def __init__(self, input_dim, window_size, output_dim, hidden_dim, inner_edge, outer_edge, company_to_sector):\n",
    "        super(CategoricalGraphAtt, self).__init__()\n",
    "\n",
    "        # basic parameters\n",
    "        self.input_dim = input_dim      # feature_size\n",
    "        self.window_size = window_size\n",
    "        self.output_dim = output_dim    # cum_labels + 1\n",
    "        self.hidden_dim = hidden_dim \n",
    "        self.inner_edge = inner_edge    # expected dimension: (2, num_edges)\n",
    "        self.outer_edge = outer_edge    # expected dimension: (2, num_edges)\n",
    "        self.company_to_sector = company_to_sector # dictionary to map each company to its sector\n",
    "        self.num_sector = len(set(company_to_sector.values()))\n",
    "        self.num_company = len(set(company_to_sector.keys()))\n",
    "\n",
    "        # hidden layers\n",
    "        self.bn = nn.BatchNorm1d(input_dim * window_size, momentum=None) # Batch Normalization Layer\n",
    "        # self.sequence_encoder = nn.GRU(input_size=input_dim, hidden_size=hidden_dim)\n",
    "        self.sequence_encoder = Graph_GRUModel(self.num_company, input_dim, hidden_dim)\n",
    "        self.inner_gat = GATConv(hidden_dim, hidden_dim) # GAT to conduct intra-sector relation\n",
    "        self.cat_gat = GATConv(hidden_dim, hidden_dim) # GAT to conduct inter-sector relation\n",
    "        self.fusion = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "\n",
    "        # output layer\n",
    "        self.logit_f = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
    "\n",
    "    def sector_separater(self, full_embeddings):\n",
    "        '''\n",
    "        Separate the embeddings into different sectors based on a given mapping.\n",
    "\n",
    "            Args:\n",
    "            - full_embeddings (torch.Tensor): The embeddings tensor with shape (num_entities, hidden_dim) \n",
    "                                            representing all entities.\n",
    "\n",
    "            Returns:\n",
    "            - sectors_embeddings (dict): A dictionary where keys are sector identifiers and values are \n",
    "                                        tensors containing embeddings of all entities belonging to that sector.\n",
    "        '''\n",
    "\n",
    "        # explicit mapping to ensure each company is mapped to the correct sector\n",
    "        sectors_embeddings = {} # Dictionary to store each company's inner_graph_embedding for each sector\n",
    "        for company_idx, embedding in enumerate(full_embeddings):\n",
    "            sector = self.company_to_sector[company_idx]\n",
    "\n",
    "            if sector not in sectors_embeddings:\n",
    "                sectors_embeddings[sector] = []\n",
    "\n",
    "            sectors_embeddings[sector].append(embedding)\n",
    "        \n",
    "        # Convert lists to tensor\n",
    "        for sector, embeddings in sectors_embeddings.items():\n",
    "            sectors_embeddings[sector] = torch.stack(embeddings)\n",
    "            # print(f\"Shape of sector {sector}\", sectors_embeddings[sector].shape)\n",
    "            \n",
    "        return sectors_embeddings\n",
    "\n",
    "    def forward(self, daily_data_batch):\n",
    "        print(\"Shape of daily_data_batch\", daily_data_batch.shape)\n",
    "        sequence_embeddings = self.sequence_encoder(daily_data_batch)\n",
    "        print(\"Shape of sequence_embeddings:\", sequence_embeddings.shape)\n",
    "\n",
    "        # Conduct intra-sector embedding\n",
    "        intra_sector_embeddings = self.inner_gat(sequence_embeddings, self.inner_edge)\n",
    "        print(\"Shape of intra_sector_embeddings:\", intra_sector_embeddings.shape)\n",
    "\n",
    "        # MaxPool to get each sector's embedding\n",
    "        sectors_embeddings = self.sector_separater(intra_sector_embeddings)\n",
    "        for sector, embeddings in sectors_embeddings.items():\n",
    "            sectors_embeddings[sector], _ = torch.max(sectors_embeddings[sector], dim=0) # simply adopt MaxPool on the sectors_embeddings of each sector\n",
    "\n",
    "        sectors_embeddings = [sectors_embeddings[sector] for sector in sorted(sectors_embeddings.keys())]\n",
    "        sectors_embeddings = torch.stack(sectors_embeddings, dim=0) # (num_sectors, hidden_dim)\n",
    "        print(\"Shape of sectors_embeddings:\", sectors_embeddings.shape)\n",
    "        \n",
    "        # Conduct inter-sector embedding\n",
    "        sectors_embeddings = self.cat_gat(sectors_embeddings, self.outer_edge) # (num_sectors, hidden_dim)\n",
    "        print(\"Shape of sectors_embeddings:\", sectors_embeddings.shape)\n",
    "\n",
    "        # fusion\n",
    "\n",
    "        # duplicate sector embeddings for fusion\n",
    "        sector_counts = Counter(self.company_to_sector.values())\n",
    "        rep = torch.tensor([sector_counts[i] for i in sorted(sector_counts.keys())])  # Create repetitions tensor\n",
    "        sectors_embeddings = torch.repeat_interleave(sectors_embeddings, rep, dim=0)\n",
    "        print(\"Shape of sectors_embeddings:\", sectors_embeddings.shape)\n",
    "\n",
    "        fusion_vec = torch.cat((sequence_embeddings, sectors_embeddings, intra_sector_embeddings), dim=-1)\n",
    "        fusion_vec = torch.relu(self.fusion(fusion_vec))\n",
    "        print(\"Shape of fusion_vec\", fusion_vec.shape)\n",
    "\n",
    "        # output\n",
    "\n",
    "        logits = self.logit_f(fusion_vec.float())\n",
    "        logits = F.softmax(logits, dim=1)\n",
    "        logits = torch.cumsum(logits, dim=1)\n",
    "        eps = 5e-8\n",
    "        logits = torch.clamp(logits, min=eps, max=1 - eps)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class All_Company_Dataset(Dataset):\n",
    "    def __init__(self, x=None, y=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_dict = {}\n",
    "        input_dict['features'] = self.x[index]\n",
    "        input_dict['labels'] = self.y[index]\n",
    "            \n",
    "        return input_dict['features'], input_dict['labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "def set_seed(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def generate_company_ids(num_ids, start=1000, end=9999):\n",
    "    if num_ids > (end - start + 1):\n",
    "        raise ValueError(\"Cannot generate the specified number of distinct IDs in the given range.\")\n",
    "    \n",
    "    return random.sample(range(start, end + 1), num_ids)\n",
    "\n",
    "def map_companies_to_sectors(company_ids, num_entities_per_sector):\n",
    "    # Create a list of sectors based on the counts in num_entities_per_sector\n",
    "    sectors = []\n",
    "    for idx, count in enumerate(num_entities_per_sector):\n",
    "        sectors.extend([idx + 1] * count)\n",
    "\n",
    "    # Shuffle the sectors to achieve a random distribution\n",
    "    random.shuffle(sectors)\n",
    "\n",
    "    # Map company IDs to sectors\n",
    "    return {company_id: sector for company_id, sector in zip(company_ids, sectors)}\n",
    "\n",
    "def generate_inner_edges(df: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    Generate inner edges from a dataframe containing company IDs and their clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Dataframe with columns 'id' (representing company ID) and 'Cluster'.\n",
    "    \n",
    "    Returns:\n",
    "    - list: List of tuples representing the inner edges between companies in the same cluster.\n",
    "    \"\"\"\n",
    "    inner_edges = []\n",
    "\n",
    "    # Group by 'Cluster' and create edges\n",
    "    for _, group in df.groupby('Cluster'):\n",
    "        companies = group['id'].tolist()\n",
    "        for combo in combinations(companies, 2): # Get all pairs\n",
    "            inner_edges.append(combo)   # (a, b)\n",
    "            inner_edges.append(combo[::-1])  # (b, a)\n",
    "\n",
    "    return inner_edges\n",
    "\n",
    "def generate_outer_edges(df: pd.DataFrame) -> list:\n",
    "    outer_edges = []\n",
    "\n",
    "    # Extract unique sectors\n",
    "    sectors = df['Cluster'].unique().tolist()\n",
    "    print(f'num sectors: {len(sectors)}')\n",
    "    for combo in combinations(sectors, 2): # Get all pairs\n",
    "        outer_edges.append(combo)   # (a, b)\n",
    "        outer_edges.append(combo[::-1])  # (b, a)\n",
    "    \n",
    "    return outer_edges\n",
    "\n",
    "def generate_pseudo_data(days, window_size, number_of_companies, feature_size):\n",
    "    # Generate x with random values between 0 and 1\n",
    "    x = np.random.rand(days, window_size, number_of_companies, feature_size)\n",
    "    x = x.astype(np.float32)\n",
    "\n",
    "    # Generate y with random values of 1 or -1\n",
    "    y_choices = [1, -1]\n",
    "    y = np.random.choice(y_choices, size=(days, number_of_companies, 9))\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def generate_pseudo_data_iterator(is_training, batch_size, feature_size, window_size, company_id_list, days):\n",
    "    company_id_list = sorted(company_id_list)\n",
    "    x, y = generate_pseudo_data(days, window_size, len(company_id_list), feature_size)\n",
    "    print(\"Shape of x:\", x.shape)\n",
    "    print(\"Shape of y:\", y.shape)\n",
    "\n",
    "    dataset = All_Company_Dataset(x=x, y=y)\n",
    "    # DataLoader will shuffle data based on the date in each epoch\n",
    "    iterator = DataLoader(dataset, batch_size=batch_size, shuffle=True if is_training else False)\n",
    "\n",
    "    return iterator\n",
    "\n",
    "\n",
    "def map_companyID_to_index(company_id_list):\n",
    "    \"\"\"\n",
    "    return a mapping dictionary from the original company IDs to the new indices\n",
    "    Note: the order of the original data should be identical to company_id_list\n",
    "    \"\"\"\n",
    "    id_to_index = {company_id: index for index, company_id in enumerate(company_id_list)}\n",
    "    return id_to_index\n",
    "\n",
    "def map_sector_to_index(sectors):\n",
    "    sector_to_index = {sector: index for index, sector in enumerate(sectors)}\n",
    "    return sector_to_index\n",
    "\n",
    "def remap_edges(id_to_index, edges):\n",
    "    remapped_edges = edges.clone()\n",
    "    remapped_edges[0] = torch.tensor([id_to_index[original_id.item()] for original_id in edges[0]])\n",
    "    remapped_edges[1] = torch.tensor([id_to_index[original_id.item()] for original_id in edges[1]])\n",
    "    return remapped_edges\n",
    "\n",
    "def remap_company_to_sector(company_to_sector, companyID_to_index, sector_to_index):\n",
    "    remapped_dict = {}\n",
    "\n",
    "    for company, sector in company_to_sector.items():\n",
    "        company_idx = companyID_to_index.get(company)\n",
    "        sector_idx = sector_to_index.get(sector)\n",
    "\n",
    "        # Check if the company and sector have corresponding indices\n",
    "        if company_idx is None:\n",
    "            raise ValueError(f\"No index found for company '{company}' in companyID_to_index.\")\n",
    "        if sector_idx is None:\n",
    "            raise ValueError(f\"No index found for sector '{sector}' in sector_to_index.\")\n",
    "\n",
    "        remapped_dict[company_idx] = sector_idx\n",
    "        \n",
    "    return remapped_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate pseudo_data\n",
    "num_entity = 15786\n",
    "company_cluster = pd.read_csv('/tmp2/cwlin/explainable_credit/Att_NeuDP_GAT/data/company_cluster.csv')\n",
    "company_ids = pd.read_csv('/tmp2/cwlin/explainable_credit/Att_NeuDP_GAT/data/all_company_ids.csv').id.tolist()\n",
    "\n",
    "sectors = list(sorted(company_cluster.Cluster.unique()))\n",
    "company_cluster_dict = dict()\n",
    "for id, cluster in zip(company_cluster.id, company_cluster.Cluster):\n",
    "    company_cluster_dict[id] = cluster\n",
    "\n",
    "companyID_to_index = map_companyID_to_index(company_ids)\n",
    "sector_to_index = map_sector_to_index(sectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num sectors: 9\n",
      "Shape of inner_edge: torch.Size([2, 50235366])\n",
      "Shape of outer_edge: torch.Size([2, 72])\n"
     ]
    }
   ],
   "source": [
    "inner_edge = generate_inner_edges(company_cluster)\n",
    "outer_edge = generate_outer_edges(company_cluster)\n",
    "\n",
    "inner_edge = torch.tensor(inner_edge, dtype=torch.int64).t() # (2, num_edges)\n",
    "outer_edge = torch.tensor(outer_edge, dtype=torch.int64).t() # (2, num_edges)\n",
    "\n",
    "print(\"Shape of inner_edge:\", inner_edge.shape)\n",
    "print(\"Shape of outer_edge:\", outer_edge.shape)\n",
    "\n",
    "torch.save(inner_edge, 'inner_edge.pt')\n",
    "torch.save(outer_edge, 'outer_edge.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = 14\n",
    "window_size = 12\n",
    "batch_size = 1\n",
    "hidden_dim = 32\n",
    "cum_labels = 8\n",
    "\n",
    "device = torch.device('cpu')  # Device to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_edge_idx = remap_edges(companyID_to_index, inner_edge)\n",
    "outer_edge_idx = remap_edges(sector_to_index, outer_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_to_sector_idx = remap_company_to_sector(company_cluster_dict, companyID_to_index, sector_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(inner_edge_idx, 'inner_edge_idx.pt')\n",
    "torch.save(outer_edge_idx, 'outer_edge_idx.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(company_to_sector_idx, 'company_to_sector_idx.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalGraphAtt(\n",
       "  (bn): BatchNorm1d(168, eps=1e-05, momentum=None, affine=True, track_running_stats=True)\n",
       "  (sequence_encoder): Graph_GRUModel(\n",
       "    (gru_cell): Graph_GRUCell(\n",
       "      (x2h): Graph_Linear()\n",
       "      (h2h): Graph_Linear()\n",
       "    )\n",
       "  )\n",
       "  (inner_gat): GATConv(32, 32, heads=1)\n",
       "  (cat_gat): GATConv(32, 32, heads=1)\n",
       "  (fusion): Linear(in_features=96, out_features=32, bias=True)\n",
       "  (logit_f): Linear(in_features=32, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CategoricalGraphAtt(feature_size, window_size, cum_labels+1, hidden_dim, inner_edge_idx, outer_edge_idx, company_to_sector_idx)\n",
    "# Move model and data to the specified device (CPU for this example)\n",
    "model.to(device)\n",
    "model.to(torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/325 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of targets: torch.Size([15786, 9])\n",
      "Shape of daily_data_batch torch.Size([12, 15786, 14])\n",
      "Shape of sequence_embeddings: torch.Size([15786, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/325 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of intra_sector_embeddings: torch.Size([15786, 32])\n",
      "Shape of sectors_embeddings: torch.Size([9, 32])\n",
      "Shape of sectors_embeddings: torch.Size([9, 32])\n",
      "Shape of sectors_embeddings: torch.Size([15786, 32])\n",
      "Shape of fusion_vec torch.Size([15786, 32])\n",
      "shape of predict: torch.Size([15786, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train_iterator = generate_pseudo_data_iterator(True, batch_size, feature_size, window_size, company_ids, days=30)\n",
    "\n",
    "\n",
    "def load_dataset(is_training, filename, batch_size, feature_size, window_size, company_id_list):\n",
    "    # Check the compression\n",
    "    compression = \"gzip\" if \".gz\" in filename else None\n",
    "    # Get infos, features, and labels (No for_column)\n",
    "    # Read the data & skip the header\n",
    "    all_df = pd.read_csv(filename, compression=compression, header=0)    \n",
    "\n",
    "    # Fill missing values\n",
    "    features_coverage = 2 + feature_size * window_size\n",
    "    all_df.iloc[:, :2] = all_df.iloc[:, :2].fillna(\"\") # filled missing values in info columns (date, id) with an empty string\n",
    "    all_df.iloc[:, 2:features_coverage] = all_df.iloc[:, 2:features_coverage].fillna(0.0) # feature_df\n",
    "    all_df.iloc[:, features_coverage:] = all_df.iloc[:, features_coverage:].fillna(0) # label_df\n",
    "    \n",
    "    # Replace other events as 0: 2 -> 0\n",
    "    all_df.iloc[:, features_coverage:] = all_df.iloc[:, features_coverage:].replace(2, 0) # label_df\n",
    "    \n",
    "    # get all features\n",
    "    x, y = [], []\n",
    "    results_dict = dict()\n",
    "\n",
    "    date_group = all_df.groupby('date')\n",
    "    for date in all_df.date.sort_values().unique():\n",
    "        df = date_group.get_group(date)\n",
    "        df_date_id = df.sort_values(by='id').set_index('id')\n",
    "\n",
    "        # create rows with all companies fill with 0 if no data exists else fill with original data\n",
    "        df_all_company_at_t = pd.DataFrame(0, index=company_id_list, columns=df.columns)\n",
    "        df_all_company_at_t.loc[df_date_id.index, :] = df_date_id # fill original data to df_all_company_at_t if value exists\n",
    "        df_all_company_at_t['id'] = df_all_company_at_t.index\n",
    "\n",
    "        # extracts label values from df_all_company_at_t\n",
    "        label_df = df_all_company_at_t.loc[:, [\"y_cum_{:02d}\".format(h) for h in range(1, 1+8)]]\n",
    "        label_df['y_cum_09'] = 1 # every company will default in the infinite future\n",
    "        label_df.loc[label_df.index.difference(df_date_id.index), :] = -1\n",
    "        label = np.array(label_df.values, dtype=np.int32)\n",
    "\n",
    "        df_all_company_at_t.index = range(len(df_all_company_at_t))\n",
    "        results_dict[date] = df_all_company_at_t.loc[df_all_company_at_t.id.isin(df_date_id.index), :][['date', 'id']]\n",
    "\n",
    "        # time-lagged observations at time t-delta+1, ... t-1, where delta can be 1,6,12\n",
    "        feature_window = []\n",
    "        for rnn_length in range(1, window_size+1):\n",
    "            # feature\n",
    "            feature_df = df_all_company_at_t.loc[:, ['x_fea_{:02d}_w_{:02d}'.format(feat_i, rnn_length) for feat_i in range(1, feature_size+1)]]\n",
    "            feature = np.array(feature_df.values, dtype=np.float32)\n",
    "            feature_window.append(feature)\n",
    "        feature_window = np.stack(feature_window, axis=0)\n",
    "\n",
    "        x.append(feature_window) # 325 * (6, 15786, 14)\n",
    "        y.append(label)         # 325 * (15786, 9)\n",
    "        \n",
    "    x = np.stack(x) # (325, 6, 15786, 14)\n",
    "    y = np.stack(y) # (325, 15786, 9)\n",
    "\n",
    "    dataset = All_Company_Dataset(x=x, y=y)\n",
    "    iterator = DataLoader(dataset, batch_size=batch_size, shuffle=True if is_training else False)\n",
    "    return iterator, results_dict\n",
    "\n",
    "data_dir = '/tmp2/cwlin/explainable_credit/explainable_credit_new/data'\n",
    "train_data = os.path.join(data_dir, 'train_cum.csv')\n",
    "\n",
    "train_inputs, _ = load_dataset(True, train_data, batch_size, feature_size, window_size, company_ids)\n",
    "\n",
    "t = trange(len(train_inputs))\n",
    "for i, (features, labels) in zip(t, train_inputs):\n",
    "    # print(\"Shape of features\", features.shape)\n",
    "    # print(\"Shape of labels\", labels.shape)\n",
    "    batch_size = features.size(0)\n",
    "    # print(batch_size)\n",
    "    for batch in range(batch_size):\n",
    "        inputs = features[batch].to(torch.float32)\n",
    "        targets = labels[batch].to(device)\n",
    "        print(\"shape of targets:\", targets.shape)\n",
    "        predict = model(inputs)\n",
    "        print(\"shape of predict:\", predict.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn4rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
